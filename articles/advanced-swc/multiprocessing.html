<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.4: http://docutils.sourceforge.net/" />
<title>Packages for Multiprocessing</title>
<style type="text/css">

/*
:Author: David Goodger
:Contact: goodger@users.sourceforge.net
:Date: $Date: 2005-12-18 01:56:14 +0100 (Sun, 18 Dec 2005) $
:Revision: $Revision: 4224 $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin-left: 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left {
  clear: left }

img.align-right {
  clear: right }

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font-family: serif ;
  font-size: 100% }

pre.literal-block, pre.doctest-block {
  margin-left: 2em ;
  margin-right: 2em ;
  background-color: #eeeeee }

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

tt.docutils {
  background-color: #eeeeee }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="packages-for-multiprocessing">
<h1 class="title">Packages for Multiprocessing</h1>
<div class="section">
<h1><a id="threading" name="threading">threading</a></h1>
<p>Python has basic support for threading built in: for example, here's a
program that runs two threads, each of which prints out messages after
sleeping a particular amount of time:</p>
<pre class="literal-block">
from threading import Thread, local
import time

class MessageThread(Thread):
    def __init__(self, message, sleep):
        self.message = message
        self.sleep = sleep
        Thread.__init__(self)                # remember to run Thread init!

    def run(self):                           # automatically run by 'start'
        i = 0
        while i &lt; 50:
            i += 1
            print i, self.message

            time.sleep(self.sleep)

t1 = MessageThread(&quot;thread - 1&quot;, 1)
t2 = MessageThread(&quot;thread - 2&quot;, 2)

t1.start()
t2.start()
</pre>
<p>However, due to the existence of the Global Interpreter Lock (GIL)
(<a class="reference" href="http://docs.python.org/api/threads.html">http://docs.python.org/api/threads.html</a>), CPU-intensive code will
not run faster on dual-core CPUs than it will on single-core CPUs.</p>
<p>Briefly, the idea is that the Python interpreter holds a global lock,
and no Python code can be executed without holding that lock.  (Code
execution will still be interleaved, but no two Python instructions
can execute at the same time.) Therefore, any Python code that you
write (or GIL-naive C/C++ extension code) will not take advantage of
multiple CPUs.</p>
<p>This is intentional:</p>
<blockquote>
<a class="reference" href="http://mail.python.org/pipermail/python-3000/2007-May/007414.html">http://mail.python.org/pipermail/python-3000/2007-May/007414.html</a></blockquote>
<p>There is a long history of wrangling about the GIL, and there are a couple
of good arguments for it.  Briefly,</p>
<blockquote>
<ul class="simple">
<li>it dramatically simplifies writing C extension code, because by
default, C extension code does not need to know anything about
threads.</li>
<li>putting in locks appropriately to handle places where contention
might occur is not only error-prone but makes the code quite slow;
locks really affect performance.</li>
<li>threaded code is difficult to debug, and most people don't need it,
despite having been brainwashed to think that they do ;).</li>
</ul>
</blockquote>
<p>But we don't care about that: <em>we</em> do want our code to run on multiple
CPUs.  So first, let's dip back into C code: what do we have to do to
make our C code release the GIL so that it can do a long computation?</p>
<p>Basically, just wrap I/O blocking code or CPU-intensive code in the
following macros:</p>
<pre class="literal-block">
Py_BEGIN_ALLOW_THREADS

...Do some time-consuming operation...

Py_END_ALLOW_THREADS
</pre>
<p>This is actually pretty easy to do to your C code, and it does result
in that code being run in parallel on multi-core CPUs.  (note:
example?)</p>
<p>The big problem with the GIL, however, is that it really means that you
simply can't write parallel code in Python without jumping through some
kind of hoop.  Below, we discuss a couple of these hoops ;).</p>
</div>
<div class="section">
<h1><a id="writing-and-indicating-threadsafe-c-extensions" name="writing-and-indicating-threadsafe-c-extensions">Writing (and indicating) threadsafe C extensions</a></h1>
<p>Suppose you had some CPU-expensive C code:</p>
<pre class="literal-block">
void waste_time() {
     int i, n;
     for (i = 0; i &lt; 1024*1024*1024; i++) {
         if ((i % 2) == 0) n++;
     }
}
</pre>
<p>and you wrapped this in a Python function:</p>
<pre class="literal-block">
PyObject * waste_time_fn(PyObject * self, PyObject * args) {
     waste_time();
}
</pre>
<p>Now, left like this, any call to <tt class="docutils literal"><span class="pre">waste_time_fn</span></tt> will cause all
Python threads and processes to block, waiting for <tt class="docutils literal"><span class="pre">waste_time</span></tt> to
finish.  That's silly, though -- <tt class="docutils literal"><span class="pre">waste_time</span></tt> is clearly threadsafe,
because it uses only local variables!</p>
<p>To tell Python that you are engaged in some expensive operations that
are threadsafe, just enclose the waste_time code like so:</p>
<pre class="literal-block">
PyObject * waste_time_fn(PyObject * self, PyObject * args) {
     Py_BEGIN_ALLOW_THREADS

     waste_time();

     Py_END_ALLOW_THREADS
}
</pre>
<p>This code will now be run in parallel when threading is used.  One
caveat: you can't do <em>any</em> call to the Python C API in the code
between the Py_BEGIN_ALLOW_THREADS and Py_END_ALLOW_THREADS, because
the Python C API is not threadsafe.</p>
</div>
<div class="section">
<h1><a id="parallelpython" name="parallelpython">parallelpython</a></h1>
<p>parallelpython is a system for controlling multiple Python processes on
multiple machines.  Here's an example program:</p>
<pre class="literal-block">
#!/usr/bin/python
def isprime(n):
    &quot;&quot;&quot;Returns True if n is prime and False otherwise&quot;&quot;&quot;
    import math

    if n &lt; 2:
        return False
    if n == 2:
        return True
    max = int(math.ceil(math.sqrt(n)))
    i = 2
    while i &lt;= max:
        if n % i == 0:
            return False
        i += 1
    return True

def sum_primes(n):
    &quot;&quot;&quot;Calculates sum of all primes below given integer n&quot;&quot;&quot;
    return sum([x for x in xrange(2, n) if isprime(x)])

####

import sys, time

import pp
# Creates jobserver with specified number of workers
job_server = pp.Server(ncpus=int(sys.argv[1]))

print &quot;Starting pp with&quot;, job_server.get_ncpus(), &quot;workers&quot;

start_time = time.time()

# Submit a job of calulating sum_primes(100) for execution.
#
#    * sum_primes - the function
#    * (input,) - tuple with arguments for sum_primes
#    * (isprime,) - tuple with functions on which sum_primes depends
#
# Execution starts as soon as one of the workers will become available

inputs = (100000, 100100, 100200, 100300, 100400, 100500, 100600, 100700)

jobs = []
for input in inputs:
    job = job_server.submit(sum_primes, (input,), (isprime,))
    jobs.append(job)

for job, input in zip(jobs, inputs):
    print &quot;Sum of primes below&quot;, input, &quot;is&quot;, job()

print &quot;Time elapsed: &quot;, time.time() - start_time, &quot;s&quot;
job_server.print_stats()
</pre>
<p>If you add &quot;ppservers=('host1')&quot; to to the line</p>
<pre class="literal-block">
pp.Server(...)
</pre>
<p>pp will check for parallelpython servers running on those other hosts and
send jobs to them as well.</p>
<p>The way parallelpython works is it literally sends the Python code across
the network &amp; evaluates it there!  It seems to work well.</p>
</div>
<div class="section">
<h1><a id="rpyc" name="rpyc">Rpyc</a></h1>
<p><a class="reference" href="http://rpyc.wikispaces.com/">Rpyc</a> is a remote procedure call system
built in (and tailored to) Python.  It is basically a way to transparently
control remote Python processes.  For example, here's some code that will
connect to an Rpyc server and ask the server to calculate the first
500 prime numbers:</p>
<pre class="literal-block">
from Rpyc import SocketConnection

# connect to the &quot;remote&quot; server
c = SocketConnection(&quot;localhost&quot;)

# make sure it has the right code in its path
c.modules.sys.path.append('/u/t/dev/misc/rpyc')

# tell it to execute 'primestuff.get_n_primes'
primes = c.modules.primestuff.get_n_primes(500)
print primes[-20:]
</pre>
<p>Note that this is a synchronous connection, so the client waits for the
result; you could also have it do the computation asynchronously, leaving
the client free to request results from other servers.</p>
<p>In terms of parallel computing, the server has to be controlled
directly, which makes it less than ideal.  I think parallelpython
is a better choice for straightforward number crunching.</p>
</div>
<div class="section">
<h1><a id="pympi" name="pympi">pyMPI</a></h1>
<p>pyMPI is a nice Python implementation to the MPI (message-passing
interface) library.  MPI enables different processors to communicate
with each other.  I can't demo pyMPI, because I couldn't get it to
work on my other machine, but here's some example code that computs pi
to a precision of 1e-6 on however many machines you have running MPI.</p>
<pre class="literal-block">
import random
import mpi

def computePi(nsamples):
    rank, size = mpi.rank, mpi.size
    oldpi, pi, mypi = 0.0,0.0,0.0

    done = False
    while(not done):
        inside = 0
        for i in xrange(nsamples):
            x = random.random()
            y = random.random()
            if ((x*x)+(y*y)&lt;1):
                inside+=1

        oldpi = pi
        mypi = (inside * 1.0)/nsamples
        pi =  (4.0 / mpi.size) * mpi.allreduce(mypi, mpi.SUM)

        delta = abs(pi - oldpi)
        if(mpi.rank==0):
            print &quot;pi:&quot;,pi,&quot; - delta:&quot;,delta
        if(delta &lt; 0.00001):
            done = True
    return pi

if __name__==&quot;__main__&quot;:
    pi = computePi(10000)
    if(mpi.rank==0):
        print &quot;Computed value of pi on&quot;,mpi.size,&quot;processors is&quot;,pi
</pre>
<p>One big problem with MPI is that documentation is essentially absent, but
I can still make a few points ;).</p>
<p>First, the &quot;magic&quot; happens in the 'allreduce' function up above, where
it sums the results from all of the machines and then divides by the
number of machines.</p>
<p>Second, pyMPI takes the unusual approach of actually building an
MPI-aware Python interpreter, so instead of running your scripts in
normal Python, you run them using 'pyMPI'.</p>
</div>
<div class="section">
<h1><a id="multitask" name="multitask">multitask</a></h1>
<p>multitask is not a multi-machine mechanism; it's a library that
implements cooperative multitasking around I/O operations.  Briefly,
whenever you're going to do an I/O operation (like wait for more
data from the network) you can tell multitask to yield to another
thread of control.  Here is a simple example where control is voluntarily
yielded after a 'print':</p>
<pre class="literal-block">
import multitask

 def printer(message):
     while True:
         print message
         yield

 multitask.add(printer('hello'))
 multitask.add(printer('goodbye'))
 multitask.run()
</pre>
<p>Here's another example from the home page:</p>
<pre class="literal-block">
import multitask

def listener(sock):
    while True:
        conn, address = (yield multitask.accept(sock))    # WAIT
        multitask.add(client_handler(conn))

def client_handler(sock):
    while True:
        request = (yield multitask.recv(sock, 1024))      # WAIT
        if not request:
            break
        response = handle_request(request)
        yield multitask.send(sock, response)              # WAIT

multitask.add(listener(sock))
multitask.run()
</pre>
</div>
</div>
</body>
</html>
